{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6lW8YPm7EI-5"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1882311955.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mmport time\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "mport time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hel72wn-Gadt"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOour0n-pd3I"
   },
   "source": [
    "# Autó-regresszív modellek\n",
    "\n",
    "Az autó-regresszív generálási folyamat lassú, hiszen tokenenként prediktál a modell. Minden új predikció függ az előző kontextustól. Ahhoz, hogy az 1000. tokent kiszámítsuk, a modellnek szüksége van információra az előző 999 tokenről. Ahhoz, hogy az 1001. tokent kiszámítsuk, a modellnek ismét szüksége van információra az első 999 tokenről, viszont az egyetlen új művelet ezekben az 1000. tokennel való műveletek, az összes többi ismétlődik.\n",
    "\n",
    "A kulcs-érték (key-value, KV) vektorokat használjuk a figyelmi értékek kiszámításához, és a KV cache (gyorsítótár) megszünteti ezt a hatékonyságcsökkenést olyan módon, hogy tárolja a korábban feldolgozott tokenek figyelemrétegeiből származó kulcs-érték párokat.\n",
    "\n",
    "Megjegyzés. A gyorsítótárazás csak következtetési idő alatt használandó, tanítás alatt nem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ocBcSJ24Osv"
   },
   "source": [
    "# Figyelmi mátrixok\n",
    "\n",
    "A skálázott skalár-szorzat alapú figyelmi mérték az alábbi képlet alapján számolódik ki, ahol $T$ (eddigi) szekvenciahosszhoz, és $d_{\\text{h}}$ a figyelemfejek dimenziója, $b$\n",
    "a batchek mérete, $h$ a figyelemfejek száma.\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{Q K^\\top}{\\sqrt{d_{\\text{h}}}} \\times \\text{mask} \\right) V\n",
    "$$\n",
    "\n",
    "A lekérdezési (query, $Q$), kulcs (key, $K$) és érték (value, $V$) mátrixok a $(b, h, T, d_{\\text{h}})$ alakú bemeneti beágyazásokból származó vetületek.\n",
    "\n",
    "Az ok-okozati figyelem esetében a maszk megakadályozza, hogy a modell a jövőbeli tokenekre figyeljen. Miután egy token feldolgozásra került, annak reprezentációja a jövőbeli tokenek tekintetében soha nem változik, ami azt jelenti, hogy a $K_{\\text{past}}$ és a $V_{\\text{past}}$ gyorsítótárba helyezhető és újra felhasználható az utolsó token reprezentációjának kiszámításához.\n",
    "\n",
    "$$\n",
    "K_{\\text{cache}} \\leftarrow \\text{concat}(K_{\\text{past}}, k_t), \\quad V_{\\text{cache}} \\leftarrow \\text{concat}(V_{\\text{past}}, v_t)\n",
    "$$\n",
    "\n",
    "Megjegyzés. A figyelem a modell minden rétegében függetlenül kerül kiszámításra, és a gyorsítótárazás rétegenként történik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nx2NiForERsi"
   },
   "outputs": [],
   "source": [
    "class MinimalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, head_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.query = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.key = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.value = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        kv_cache: tuple[torch.Tensor, torch.Tensor] | None = None\n",
    "    ) -> tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]:\n",
    "        _, seq_len, _ = x.shape\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            k_past, v_past = kv_cache\n",
    "            k = torch.cat([k_past, k], dim=1)\n",
    "            v = torch.cat([v_past, v], dim=1)\n",
    "\n",
    "        current_cache = (k, v)\n",
    "\n",
    "        scores = (q @ k.transpose(1, 2)) / (self.head_dim**0.5)\n",
    "\n",
    "        if seq_len > 1:\n",
    "            mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device))\n",
    "            scores = scores.masked_fill(mask == 0, -torch.inf)\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        context = attn @ v\n",
    "\n",
    "        return self.o_proj(context), current_cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BE1eT9_38bz7"
   },
   "source": [
    "# Számítási igény\n",
    "\n",
    "Így a számítási igény $O(T^2)$-ről $O(T)$-re (lépésenként $O(1)$-re) csökken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQTAVa-4FjW9"
   },
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "head_dim = 64\n",
    "prompt_len = 100\n",
    "gen_steps = 200\n",
    "\n",
    "model = MinimalSelfAttention(d_model, head_dim=64).to(device)\n",
    "prompt = torch.randn(1, prompt_len, d_model).to(device)\n",
    "\n",
    "naive_times = []\n",
    "curr_input = prompt.clone()\n",
    "start_total = time.time()\n",
    "\n",
    "for i in range(gen_steps):\n",
    "    step_start = time.time()\n",
    "    _ = model(curr_input, kv_cache=None)\n",
    "\n",
    "    next_tok = torch.randn(1, 1, d_model).to(device)\n",
    "    curr_input = torch.cat([curr_input, next_tok], dim=1)\n",
    "\n",
    "    naive_times.append(time.time() - start_total)\n",
    "\n",
    "cached_times = []\n",
    "\n",
    "curr_input_c = prompt.clone()\n",
    "_, cache = model(curr_input_c, kv_cache=None)\n",
    "\n",
    "next_tok = torch.randn(1, 1, d_model).to(device)\n",
    "start_total = time.time()\n",
    "\n",
    "for i in range(gen_steps):\n",
    "    step_start = time.time()\n",
    "    _, cache = model(next_tok, kv_cache=cache)\n",
    "\n",
    "    next_tok = torch.randn(1, 1, d_model).to(device)\n",
    "    cached_times.append(time.time() - start_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "bxzWAHuZHgMY",
    "outputId": "28300be3-4a72-4f52-ae86-a61a55a9ac86"
   },
   "outputs": [],
   "source": [
    "x_axis = np.arange(1, gen_steps + 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(x_axis, naive_times, label='Naiv')\n",
    "ax.plot(x_axis, cached_times, label='KV gyorsítótár')\n",
    "\n",
    "\n",
    "ax.set_title(f'Valós futási idő mérése ({device})')\n",
    "ax.set_xlabel('Generált tokenek száma')\n",
    "ax.set_ylabel('Kumulatív idő (másodperc)')\n",
    "\n",
    "ax.scatter(gen_steps, naive_times[-1])\n",
    "ax.scatter(gen_steps, cached_times[-1])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omnF_Ifp_9HW"
   },
   "source": [
    "# Matematikai determinizmus\n",
    "\n",
    "Egy $t$ pozícióban lévő token ($x_t$) reprezentációjának kiszámításakor a modell szigorúan csak az $x_1, \\dots, x_t$ tokeneket \"láthatja\" (lásd, kauzális maszk). A jövőbeli $x_{t+1}$ token generálása vagy jelenléte semmilyen módon nem befolyásolja visszamenőleg az $x_t$ reprezentációját.\n",
    "\n",
    "Tekintsük a kulcs ($k_t$) kiszámítását:\n",
    "\n",
    "$$\n",
    "k_t = x_t W_K\n",
    "$$\n",
    "\n",
    "ahol:\n",
    "\n",
    "- $x_t$ a $t$-edik bemeneti token beágyazása (konstans),\n",
    "- $W_K$ a súlymátrix (inferencia alatt fagyasztott, konstans).\n",
    "\n",
    "Mivel a bemenet a múltban konstans, és a súlyok is konstansok, ezért a $k_t$ és a $v_t$ vektorok determinisztikusak és időben invariánsak.\n",
    "\n",
    "Megjegyzés. Ez \"Decoder-only\" modellekre igaz csak. Az \"Encoder\" típusú modelleknél, ahol a figyelem kétirányú, a jövőbeli szavak megváltoztatják a múltbeli szavak kontextusát, így ott a KV gyorsítótár nem alkalmazható."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6ux12-QC9De"
   },
   "source": [
    "# Kompromisszum\n",
    "\n",
    "Bár a KV gyorsítótár drasztikusan csökkenti a számítási időt, ennek ára a megnövekedett memóriaigény. A gyorsítótár mérete lineárisan nő a szekvencia hosszával és a batch méretével.\n",
    "\n",
    "Egyetlen réteg gyorsítótárának mérete:\n",
    "\n",
    "$$\n",
    "N_{\\text{elem}} = 2 \\times b \\times T \\times h \\times d_{\\text{h}}\n",
    "$$\n",
    "\n",
    "Ahol a $2$-es szorzó a $K$ és $V$ mátrixokat jelöli. Hosszú kontextusú modelleknél ez a gyorsítótár akár több tíz gigabájt memóriát is felemészthet.\n",
    "\n",
    "Megjegyzés. A számításhoz fizikailag mindig szükség van a $K$ és $V$ vektorokra, viszont a naív módszernél ez átmeneti memória, míg a KV gyorsítótár idejében tartós, ami folyamatosan foglalja a VRAM-ot, csökkentve ezzel a párhuzamosan kiszolgálható felhasználók számát."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.13.11 (~/Work/ai_notebooks/.venv)",
   "language": "python",
   "name": "ai_notebooks_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
